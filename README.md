# Mechanistic Interpretability on Modular Arithmetic

Implementing a transformer from scratch to train and study how neural networks learn tasks, specifically modular arithmetic (113). This project serves as an introduction into interpretability, aiming to build intiution as to how neural networks learn.

## Project Structure
```
modular-arithmetic-interp/
├── data/              # Make the dataset
├── models/            # Define transformer (and other architeectures)
├── training/          # Train the transformer
├── interpretability/  # Analyze what it learned
├── notebooks/         # Interactive exploration
├── results/           # Store outputs
└── experiments/       # Try extensions (attempting a mod 127 extension)
```
